{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kor.extraction import create_extraction_chain\n",
    "from kor.nodes import Object, Text, Number\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from ../models/Mixtral-8x7B-Instruct-v0.1-GGUF/mixtral-8x7b-instruct-v0.1.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:          blk.0.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:          blk.0.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:          blk.0.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:          blk.0.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:            blk.0.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:          blk.0.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:          blk.0.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:          blk.0.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:          blk.0.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.0.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:          blk.0.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:          blk.0.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:            blk.0.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:          blk.0.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:          blk.0.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.0.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.0.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:          blk.0.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:            blk.0.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:          blk.0.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:          blk.0.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:            blk.0.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.0.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.0.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:         blk.0.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.0.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.0.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:          blk.1.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:          blk.1.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.1.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:          blk.1.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:          blk.1.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:            blk.1.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:          blk.1.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:          blk.1.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:            blk.1.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:          blk.1.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:          blk.1.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.1.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:          blk.1.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:          blk.1.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:        blk.1.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.1.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:         blk.1.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.1.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.1.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:            blk.1.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:          blk.1.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:          blk.1.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.1.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.1.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:          blk.1.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:            blk.1.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:          blk.1.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:          blk.1.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:            blk.1.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:          blk.2.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.2.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.2.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:          blk.2.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:          blk.2.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.2.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:          blk.2.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:          blk.2.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.2.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:          blk.2.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.2.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.2.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:          blk.2.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:          blk.2.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.2.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:          blk.2.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:          blk.2.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.2.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:          blk.2.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.2.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.2.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:          blk.2.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:          blk.2.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.2.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:        blk.2.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.2.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:         blk.2.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:              blk.2.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:              blk.2.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:          blk.3.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:          blk.3.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:            blk.3.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:          blk.3.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.3.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:            blk.3.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.3.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:        blk.3.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:              blk.3.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:         blk.3.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:              blk.3.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:              blk.3.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:          blk.3.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:            blk.3.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:          blk.3.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.3.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.3.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:          blk.3.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:          blk.3.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.3.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:          blk.3.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.3.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:            blk.3.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:          blk.3.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.3.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:            blk.3.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:          blk.3.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:          blk.3.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:            blk.3.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.4.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:          blk.4.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:            blk.4.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:          blk.4.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:          blk.4.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:            blk.4.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:          blk.4.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:          blk.4.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:            blk.4.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.4.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:          blk.4.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:            blk.4.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:          blk.4.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:          blk.4.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:            blk.4.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:          blk.4.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:          blk.4.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:            blk.4.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.4.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:          blk.4.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:            blk.4.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:          blk.4.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:          blk.4.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:            blk.4.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:        blk.4.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:              blk.4.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:         blk.4.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:              blk.4.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:              blk.4.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:        blk.5.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.5.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:         blk.5.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.5.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:              blk.5.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.5.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:          blk.5.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:            blk.5.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:          blk.5.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:          blk.5.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:            blk.5.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:          blk.5.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:          blk.5.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:            blk.5.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.5.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:          blk.5.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:            blk.5.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:          blk.5.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:          blk.5.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:            blk.5.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:          blk.5.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:          blk.5.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:            blk.5.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.5.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:          blk.5.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:            blk.5.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:          blk.5.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:          blk.5.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:            blk.5.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:          blk.6.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.6.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:            blk.6.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.6.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:          blk.6.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.6.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:          blk.6.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:          blk.6.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:            blk.6.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.6.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.6.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:            blk.6.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.6.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:          blk.6.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.6.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:          blk.6.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:          blk.6.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.6.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:              blk.6.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:         blk.6.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.6.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:              blk.6.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.6.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:          blk.6.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:          blk.6.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.6.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:          blk.6.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:          blk.6.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:            blk.6.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.7.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:          blk.7.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:            blk.7.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:          blk.7.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:          blk.7.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:            blk.7.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.7.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:          blk.7.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:            blk.7.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.7.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:          blk.7.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:            blk.7.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:          blk.7.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:          blk.7.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:            blk.7.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.7.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:          blk.7.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:            blk.7.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.7.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:          blk.7.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:            blk.7.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:          blk.7.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:          blk.7.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:            blk.7.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.7.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:              blk.7.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:         blk.7.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:              blk.7.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:              blk.7.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:          blk.8.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:          blk.8.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:            blk.8.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:          blk.8.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.8.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:            blk.8.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:          blk.8.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:          blk.8.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:            blk.8.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:          blk.8.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.8.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:              blk.8.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:         blk.8.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:              blk.8.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:              blk.8.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:         blk.10.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:         blk.10.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.10.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:       blk.10.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.10.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.10.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.10.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.10.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.8.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:            blk.8.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:          blk.8.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:          blk.8.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:            blk.8.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:          blk.8.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.8.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:            blk.8.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.8.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.8.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:            blk.8.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:          blk.8.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:          blk.8.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:            blk.8.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:          blk.9.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.9.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:            blk.9.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:          blk.9.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:          blk.9.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:            blk.9.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:          blk.9.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:          blk.9.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:            blk.9.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:          blk.9.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.9.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:            blk.9.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:          blk.9.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:          blk.9.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:            blk.9.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:          blk.9.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:          blk.9.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:            blk.9.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:          blk.9.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.9.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:            blk.9.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:          blk.9.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:          blk.9.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:            blk.9.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.9.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:              blk.9.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:         blk.9.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:              blk.9.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:              blk.9.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:         blk.10.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:         blk.10.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:           blk.10.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:         blk.10.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:         blk.10.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:           blk.10.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:         blk.10.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:         blk.10.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:           blk.10.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:         blk.10.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:         blk.10.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:           blk.10.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:         blk.10.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:         blk.10.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:           blk.10.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:         blk.10.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:         blk.10.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:           blk.10.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:         blk.10.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:         blk.10.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:           blk.10.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:         blk.11.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:         blk.11.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.11.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:         blk.11.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:         blk.11.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:           blk.11.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:         blk.11.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:         blk.11.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.11.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:         blk.11.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:         blk.11.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.11.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:         blk.11.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:         blk.11.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:           blk.11.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:         blk.11.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:         blk.11.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.11.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:         blk.11.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:         blk.11.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:       blk.11.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.11.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:        blk.11.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.11.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:             blk.11.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.11.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:         blk.11.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:         blk.11.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:           blk.11.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:         blk.12.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:         blk.12.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:           blk.12.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:         blk.12.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:         blk.12.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:           blk.12.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:         blk.12.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:         blk.12.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:           blk.12.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:         blk.12.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:         blk.12.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:           blk.12.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:         blk.12.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:         blk.12.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:           blk.12.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:         blk.12.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:         blk.12.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:           blk.12.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:         blk.12.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:         blk.12.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:           blk.12.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:         blk.12.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:         blk.12.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:           blk.12.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:       blk.12.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.12.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:        blk.12.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:             blk.12.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:             blk.12.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:         blk.13.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:         blk.13.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:           blk.13.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:         blk.13.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:         blk.13.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:           blk.13.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:         blk.13.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:         blk.13.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.13.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:         blk.13.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:         blk.13.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:           blk.13.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:         blk.13.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:       blk.13.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.13.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:        blk.13.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:             blk.13.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:             blk.13.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:         blk.13.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:           blk.13.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:         blk.13.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:         blk.13.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:           blk.13.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:         blk.13.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:         blk.13.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:           blk.13.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:         blk.13.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:         blk.13.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:           blk.13.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:         blk.14.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:         blk.14.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:           blk.14.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:         blk.14.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:         blk.14.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.14.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:         blk.14.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:         blk.14.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.14.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:         blk.14.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:         blk.14.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:           blk.14.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:         blk.14.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:         blk.14.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.14.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:         blk.14.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:         blk.14.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.14.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:         blk.14.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:         blk.14.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:           blk.14.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:         blk.14.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:         blk.14.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.14.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:       blk.14.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.14.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:        blk.14.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.14.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:             blk.14.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:         blk.15.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:         blk.15.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:           blk.15.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:         blk.15.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:         blk.15.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:           blk.15.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:       blk.15.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.15.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.15.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:             blk.15.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:             blk.15.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:         blk.15.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:         blk.15.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.15.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:         blk.15.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:         blk.15.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:           blk.15.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:         blk.15.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:         blk.15.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.15.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:         blk.15.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:         blk.15.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.15.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:         blk.15.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:         blk.15.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:           blk.15.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:         blk.15.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:         blk.15.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.15.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:         blk.16.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:         blk.16.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:           blk.16.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:         blk.16.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:         blk.16.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.16.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:         blk.16.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:         blk.16.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:           blk.16.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:         blk.16.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:         blk.16.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:           blk.16.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:         blk.16.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:         blk.16.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.16.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:         blk.16.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:         blk.16.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:           blk.16.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:         blk.16.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:         blk.16.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:           blk.16.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:         blk.16.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:         blk.16.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:       blk.16.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:             blk.16.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:        blk.16.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:             blk.16.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:             blk.16.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:           blk.16.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:         blk.17.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:         blk.17.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.17.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:         blk.17.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:         blk.17.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.17.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:         blk.17.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:         blk.17.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:           blk.17.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:         blk.17.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:         blk.17.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.17.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:         blk.17.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:         blk.17.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.17.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:         blk.17.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:         blk.17.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:           blk.17.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:         blk.17.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:         blk.17.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.17.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:         blk.17.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:         blk.17.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.17.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:       blk.17.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:             blk.17.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:        blk.17.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:             blk.17.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.17.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:         blk.18.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:         blk.18.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:           blk.18.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:         blk.18.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:         blk.18.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:           blk.18.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:         blk.18.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:         blk.18.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:           blk.18.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:         blk.18.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:         blk.18.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:           blk.18.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:         blk.18.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:         blk.18.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:           blk.18.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:         blk.18.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:       blk.18.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.18.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:        blk.18.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:             blk.18.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.18.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:         blk.18.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:           blk.18.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:         blk.18.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:         blk.18.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.18.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:         blk.18.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:         blk.18.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.18.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:         blk.19.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:         blk.19.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.19.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:         blk.19.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:         blk.19.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:           blk.19.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:         blk.19.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:         blk.19.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:           blk.19.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:         blk.19.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:         blk.19.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.19.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:         blk.19.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:         blk.19.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:           blk.19.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:         blk.19.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:         blk.19.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:           blk.19.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:         blk.19.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:         blk.19.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.19.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:         blk.19.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:         blk.19.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:           blk.19.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:       blk.19.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.19.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.19.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:             blk.19.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:             blk.19.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:         blk.20.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:         blk.20.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.20.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:         blk.20.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:         blk.20.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:           blk.20.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:         blk.20.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:         blk.20.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.20.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:       blk.20.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:             blk.20.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:        blk.20.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.20.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.20.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:         blk.20.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:         blk.20.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.20.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:         blk.20.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:         blk.20.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:           blk.20.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:         blk.20.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:         blk.20.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:           blk.20.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:         blk.20.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:         blk.20.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.20.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:         blk.20.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:         blk.20.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:           blk.20.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:         blk.21.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:         blk.21.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:           blk.21.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:         blk.21.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:         blk.21.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:           blk.21.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:         blk.21.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:         blk.21.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:           blk.21.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:         blk.21.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:         blk.21.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:           blk.21.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:         blk.21.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:         blk.21.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:           blk.21.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:         blk.21.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:         blk.21.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:           blk.21.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:         blk.21.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:         blk.21.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:           blk.21.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:         blk.21.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:         blk.21.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:           blk.21.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:       blk.21.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.21.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:        blk.21.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:             blk.21.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:             blk.21.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:         blk.22.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:         blk.22.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:       blk.22.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:             blk.22.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:        blk.22.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.22.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.22.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:           blk.22.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:         blk.22.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:         blk.22.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:           blk.22.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:         blk.22.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:         blk.22.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:           blk.22.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:         blk.22.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:         blk.22.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:           blk.22.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:         blk.22.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:         blk.22.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:           blk.22.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:         blk.22.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:         blk.22.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:           blk.22.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:         blk.22.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:         blk.22.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:           blk.22.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:         blk.22.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:         blk.22.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:           blk.22.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:         blk.23.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:         blk.23.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:           blk.23.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:         blk.23.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:         blk.23.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.23.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:         blk.23.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:         blk.23.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.23.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  723:         blk.23.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  724:         blk.23.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  725:           blk.23.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  726:         blk.23.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  727:         blk.23.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  728:           blk.23.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  729:         blk.23.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  730:         blk.23.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  731:           blk.23.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  732:         blk.23.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  733:       blk.23.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  734:             blk.23.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  735:        blk.23.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  736:             blk.23.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  737:             blk.23.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  738:         blk.23.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  739:           blk.23.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  740:         blk.23.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  741:         blk.23.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  742:           blk.23.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  743:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  744:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  745:         blk.24.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  746:         blk.24.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  747:           blk.24.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  748:         blk.24.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  749:         blk.24.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  750:           blk.24.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  751:         blk.24.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  752:         blk.24.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  753:           blk.24.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  754:         blk.24.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  755:         blk.24.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  756:           blk.24.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  757:         blk.24.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  758:         blk.24.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  759:           blk.24.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  760:         blk.24.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  761:         blk.24.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  762:           blk.24.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  763:         blk.24.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  764:         blk.24.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  765:           blk.24.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  766:         blk.24.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  767:         blk.24.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  768:           blk.24.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  769:       blk.24.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  770:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  771:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  772:             blk.24.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  773:        blk.24.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  774:             blk.24.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  775:             blk.24.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  776:         blk.25.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  777:         blk.25.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  778:           blk.25.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  779:         blk.25.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  780:         blk.25.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  781:           blk.25.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  782:         blk.25.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  783:         blk.25.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  784:           blk.25.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  785:         blk.25.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  786:         blk.25.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  787:           blk.25.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  788:       blk.25.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  789:             blk.25.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  790:        blk.25.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  791:             blk.25.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  792:             blk.25.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  793:         blk.25.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  794:         blk.25.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  795:           blk.25.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  796:         blk.25.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  797:         blk.25.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  798:           blk.25.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  799:         blk.25.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  800:         blk.25.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  801:           blk.25.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  802:         blk.25.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  803:         blk.25.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  804:           blk.25.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  805:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  806:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  807:         blk.26.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  808:         blk.26.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  809:           blk.26.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  810:         blk.26.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  811:         blk.26.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  812:           blk.26.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  813:         blk.26.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  814:         blk.26.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  815:           blk.26.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  816:         blk.26.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  817:         blk.26.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  818:           blk.26.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  819:         blk.26.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  820:         blk.26.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  821:           blk.26.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  822:         blk.26.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  823:         blk.26.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  824:           blk.26.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  825:         blk.26.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  826:         blk.26.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  827:           blk.26.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  828:         blk.26.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  829:         blk.26.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  830:           blk.26.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  831:       blk.26.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  832:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  833:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  834:             blk.26.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  835:        blk.26.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  836:             blk.26.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  837:             blk.26.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  838:         blk.27.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  839:         blk.27.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  840:           blk.27.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  841:         blk.27.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  842:         blk.27.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  843:       blk.27.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  844:             blk.27.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  845:        blk.27.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  846:             blk.27.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  847:             blk.27.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  848:           blk.27.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  849:         blk.27.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  850:         blk.27.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  851:           blk.27.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  852:         blk.27.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  853:         blk.27.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  854:           blk.27.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  855:         blk.27.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  856:         blk.27.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  857:           blk.27.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  858:         blk.27.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  859:         blk.27.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  860:           blk.27.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  861:         blk.27.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  862:         blk.27.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  863:           blk.27.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  864:         blk.27.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  865:         blk.27.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  866:           blk.27.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  867:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  868:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  869:         blk.28.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  870:         blk.28.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  871:           blk.28.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  872:         blk.28.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  873:         blk.28.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  874:           blk.28.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  875:         blk.28.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  876:         blk.28.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  877:           blk.28.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  878:         blk.28.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  879:         blk.28.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  880:           blk.28.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  881:         blk.28.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  882:         blk.28.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  883:           blk.28.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  884:         blk.28.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  885:         blk.28.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  886:           blk.28.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  887:         blk.28.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  888:         blk.28.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  889:           blk.28.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  890:         blk.28.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  891:       blk.28.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  892:             blk.28.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  893:        blk.28.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  894:             blk.28.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  895:             blk.28.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  896:         blk.28.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  897:           blk.28.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  898:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  899:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  900:         blk.29.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  901:         blk.29.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  902:           blk.29.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  903:         blk.29.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  904:         blk.29.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  905:           blk.29.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  906:         blk.29.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  907:         blk.29.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  908:           blk.29.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  909:         blk.29.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  910:         blk.29.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  911:           blk.29.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  912:         blk.29.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  913:         blk.29.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  914:           blk.29.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  915:         blk.29.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  916:         blk.29.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  917:           blk.29.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  918:         blk.29.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  919:         blk.29.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  920:           blk.29.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  921:         blk.29.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  922:         blk.29.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  923:           blk.29.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  924:       blk.29.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  925:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  926:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  927:             blk.29.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  928:        blk.29.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  929:             blk.29.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  930:             blk.29.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  931:         blk.30.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  932:         blk.30.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  933:           blk.30.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  934:         blk.30.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  935:         blk.30.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  936:           blk.30.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  937:         blk.30.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  938:         blk.30.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  939:           blk.30.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  940:         blk.30.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  941:         blk.30.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  942:           blk.30.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  943:         blk.30.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  944:         blk.30.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  945:           blk.30.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  946:       blk.30.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  947:             blk.30.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  948:        blk.30.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  949:             blk.30.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  950:             blk.30.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  951:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  952:         blk.30.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  953:         blk.30.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  954:           blk.30.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  955:         blk.30.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  956:         blk.30.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  957:           blk.30.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  958:         blk.30.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  959:         blk.30.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  960:           blk.30.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  961:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  962:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  963:         blk.31.ffn_gate.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  964:         blk.31.ffn_down.0.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  965:           blk.31.ffn_up.0.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  966:         blk.31.ffn_gate.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  967:         blk.31.ffn_down.1.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  968:           blk.31.ffn_up.1.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  969:         blk.31.ffn_gate.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  970:         blk.31.ffn_down.2.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  971:           blk.31.ffn_up.2.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  972:         blk.31.ffn_gate.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  973:         blk.31.ffn_down.3.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  974:           blk.31.ffn_up.3.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  975:         blk.31.ffn_gate.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  976:         blk.31.ffn_down.4.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  977:           blk.31.ffn_up.4.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  978:         blk.31.ffn_gate.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  979:         blk.31.ffn_down.5.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  980:           blk.31.ffn_up.5.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  981:         blk.31.ffn_gate.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  982:         blk.31.ffn_down.6.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  983:           blk.31.ffn_up.6.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  984:         blk.31.ffn_gate.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  985:         blk.31.ffn_down.7.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  986:           blk.31.ffn_up.7.weight q6_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  987:       blk.31.ffn_gate_inp.weight f16      [  4096,     8,     1,     1 ]\n",
      "llama_model_loader: - tensor  988:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  989:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  990:             blk.31.attn_k.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  991:        blk.31.attn_output.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  992:             blk.31.attn_q.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  993:             blk.31.attn_v.weight q8_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  994:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q6_K:  834 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q6_K\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 35.74 GiB (6.57 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.39 MiB\n",
      "llm_load_tensors: mem required  = 36600.49 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3500\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  437.50 MiB, K (f16):  218.75 MiB, V (f16):  218.75 MiB\n",
      "llama_build_graph: non-view tensors processed: 1124/1124\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Max\n",
      "ggml_metal_init: picking default device: Apple M2 Max\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 280.93 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 36600.84 MiB, (36602.47 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   437.53 MiB, (37040.00 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   277.62 MiB, (37317.62 / 49152.00)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"../models/Mixtral-8x7B-Instruct-v0.1-GGUF/mixtral-8x7b-instruct-v0.1.Q6_K.gguf\",\n",
    "    n_gpu_layers=50,\n",
    "    n_batch=512,\n",
    "    n_ctx=3500,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_merge.csv')\n",
    "df['part_1'] = [x.split('II')[0] for x in df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема для выгрузки данных из applic, раздел I:\n",
    "* локация\n",
    "* описание тендера\n",
    "* название тендера\n",
    "* идентификатор тендера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Object(\n",
    "    id=\"tender_info\",\n",
    "    description=\"common tender information.\",\n",
    "    attributes=[\n",
    "        Text(\n",
    "            id=\"location\",\n",
    "            description=\"The territory where the tender project is implemented, which is important for determining jurisdiction and local conditions.\",\n",
    "            examples=[(\"Территория, в которой осуществляется тендерный проект: Гуанси-Маньчжурский автономный округ, Хочжоу, Чжуншань\", \"Гуанси-Маньчжурский автономный округ, Хочжоу, Чжуншань\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"product\",\n",
    "            description=\"Product or service to be provided.\",\n",
    "            examples=[(\"Условия торгов Проект строительства новой станции HLF\", \"Проект строительства новой станции HLF\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"tender_number\",\n",
    "            description=\"Number of tender\",\n",
    "            examples=[(\"номер тендера: CGN-20230324006\",\"CGN-20230324006\")]\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"tender_name\",\n",
    "            description=\"Name of tender\",\n",
    "            examples=[(\"Китайский документ 51-13trans.txt Объявление тендера на строительство новой станции HLF\",\"Объявление тендера на строительство новой станции HLF\")]\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"tender_company\",\n",
    "            description=\"The name of the company organizing the tender.\",\n",
    "            examples=[(\"организатором конкурса является компания &lt; &lt; Чайна Хинэ &gt; &gt;\", \"Чайна Хинэ\"), (\"участие в тендере компании &quot; Чайна Хинси &quot;\", \"Чайна Хинси\")]\n",
    "        ),\n",
    "    ],\n",
    "    examples=[\n",
    "        (\n",
    "            \"\"\"20230731_12345\n",
    "            Китайский документ 14-2trans.txt\n",
    "            Проект SK LOT101Df Архипелагические клапаны и системы REN\n",
    "            (Контингент: CGN-20230619005-N1)\n",
    "            - район, в котором осуществляется тендерный проект: провинция Шаньдун;\n",
    "            Условия торгов\n",
    "            SKLOT101DfRENCGN-20230619005-N1\n",
    "            Утверждены клапаны системы &lt; &lt; Ядерный остров &gt; &gt; и &lt; &lt; Система &gt; &gt; (номер тендера:\n",
    "            Источником финансирования является самофинансируемое предприятие, которое было привлечено к участию в конкурсе на участие в конкурсе компании &quot; Чайна инжиниринг лтд. &quot; . В рамках этого проекта уже имеются условия для проведения торгов и в настоящее время проводятся открытые торги.\n",
    "            \"\"\",\n",
    "            [\n",
    "                {\"location\": \"провинция Шаньдун\", \"product\": 'клапаны системы',\"tender_number\":\"CGN-20230619005-N1\",\n",
    "                 \"tender_name\": \"Архипелагические клапаны и системы REN\", \"tender_company\": \"Чайна инжиниринг лтд\"},\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"\"\"\n",
    "            20230731_12345\n",
    "            Китайский документ 85-8trans.txt\n",
    "            Глава I Уведомление о торгах\n",
    "            Район, в котором осуществляется тендерный проект: Немонгольский автономный район, УНИТА, правый флаг Корнея\n",
    "            Условия торгов\n",
    "            В рамках этого проекта &lt; &lt; Внутренняя Монголия &gt; &gt; закупила оборудование для оптических камер УНИТА (первый пункт: C, D) (заявочный номер проекта: CGN-\n",
    "            202211140006 Утверждено Уполномоченным фондом, финансируемым за счет средств, полученных от предприятий, и реализовано, а организатором конкурса является Китай.\n",
    "            Новая энергетическая компания Corp. В рамках этого проекта уже имеются условия для проведения торгов и в настоящее время проводятся открытые торги.\n",
    "            \"\"\",\n",
    "            [\n",
    "                {\"location\": \"Немонгольский автономный район, УНИТА, правый флаг Корнея\", \"product\": 'оборудование для оптических камер',\"tender_number\":\"CGN-202211140006\",\n",
    "                 \"tender_name\": \"\", \"tender_company\": \"Новая энергетическая компания\"},\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"\"\"\n",
    "            20230731_12345\n",
    "            Китайский документ 28-2trans.txt\n",
    "            Глава I. Объявление о торгах на приобретение динамической системы многопрофильной загрузки\n",
    "            (Контингент: CGN-202305230007)\n",
    "            Место проведения торгов: Гуандун\n",
    "            Условия торгов\n",
    "            Проект Закупка динамической системы многопрофильной рефлексии (закупка тендера No CGN-20230523007) завершена\n",
    "            Проверено, проект финансируется за счет собственных средств и реализован, а тендер - за счет компании China Institute of Nuclear. В рамках этого проекта уже имеется тендерная заявка\n",
    "            В настоящее время проводятся открытые торги.\n",
    "            \"\"\",\n",
    "            [\n",
    "                {\"location\": \"Гуандун\", \"product\": 'Закупка динамической системы многопрофильной рефлексии',\"tender_number\":\"CGN-202305230007\",\n",
    "                 \"tender_name\": \"Объявление о торгах на приобретение динамической системы многопрофильной загрузки\", \"tender_company\": \"China Institute of Nuclear\"},             \n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    many=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создаем объект chain с указание формата возвращаемых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_extraction_chain(llm, schema, encoder_or_encoder_class='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "через цикл обращаемся к llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i_text in tqdm(df['part_1']):\n",
    "    ans = chain.run(text=(i_text))\n",
    "    result.append(ans['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for i in tqdm(range(len(result))):\n",
    "    if len(result[i]) > 0:\n",
    "        temp_list.append(result[i]['tender_info'][0])\n",
    "    else:\n",
    "        temp_list.append({'location': '',\n",
    "                        'product': '',\n",
    "                        'tender_number': '',\n",
    "                        'tender_name': '',\n",
    "                        'tender_company': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df.to_csv('./data/part_1_BIG_MODELfor_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['part_2'] = [x.split('III')[0] for x in df['text']]\n",
    "df['part_2'] = [x.split('II')[-1] for x in df['part_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создаем схему для раздела II:\n",
    "* описание предмета тендера\n",
    "* требования к заявителю\n",
    "* этапы реализации заявки\n",
    "* период поставки товара или оказания услуг после заключения договора\n",
    "* дата поставки товара или оказания услуг\n",
    "* дата завершения поставки или оказания услуг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Object(\n",
    "    id=\"project_info\",\n",
    "    description=\"detailed information about project.\",\n",
    "    attributes=[\n",
    "        Text(\n",
    "            id=\"description\",\n",
    "            description=\"Detailed description of project.\",\n",
    "            examples=[(\"1. Масштаб проекта: проект расположен на экспериментальном заводе No 2 на промышленной базе компании &lt; &lt; Хинон-Юнк &gt; &gt; в Чжуншань, провинция Гуандун\\n Внутри. Проект строительства многосвободной системы динамического подъема требует строгой калибровки антиядерной базы и определения структурно-конструкторских составов. Дело, в котором определены требования к строительным процессам, техническая форма, например, проектная программа и проектирование архитектурных чертежей, необходимых для этого проекта.\\n2004/2005 год Этот проект закупок включает два этапа проектной работы, первый этап которой начинается после подписания контракта и второй этап - с участием участников торгов.\", \n",
    "                       \"Проект строительства многосвободной системы динамического подъема требует строгой калибровки антиядерной базы и определения структурно-конструкторских составов.\\n Дело, в котором определены требования к строительным процессам, техническая форма, например, проектная программа и проектирование архитектурных чертежей, необходимых для этого проекта.\\n2004/2005 год Этот проект закупок включает два этапа проектной работы, первый этап которой начинается после подписания контракта и второй этап - с участием участников торгов.\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"requirements\",\n",
    "            description=\"Technical and design requirements of project.\",\n",
    "            examples=[(\"Содержание и сфера охвата запроса:\\n1) Реакционная базовая программа и проектирование чертежей.\\n2) Техническое руководство по проектированию чертежей и фундаментальному строительству на месте.\", \n",
    "                       \"1) Реакционная базовая программа и проектирование чертежей.\\n2) Техническое руководство по проектированию чертежей и фундаментальному строительству на месте.\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"stages\",\n",
    "            description=\"Project stages and timelines.\",\n",
    "            examples=[(\"Этот проект закупок включает два этапа проектной работы, первый этап которой начинается после подписания контракта и второй этап - с участием участников торгов.\",\n",
    "                       \"2\"),\n",
    "                       (\"данный тендерный проект разделен на 1 пункт\", \"1\"), \n",
    "                       (\"Данный тендерный проект делится на четыре сегмента:\", '4')\n",
    "                       ]\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"delivery_time\",\n",
    "            description=\"Product delivery time.\",\n",
    "            examples=[(\"Срок поставки: срок поставки в течение 45 рабочих дней, в зависимости от того, был ли заключен контракт с участником торгов\",\"45\"), \n",
    "                      (\"Дата запланированной поставки: ожидаемая от покупателя дата поставки составляется на месяц.\", \"30\"),\n",
    "                      (\"Планируемый период\\n78 календарных дней работы\", \"78\")\n",
    "                      ]\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"start_date\",\n",
    "            description=\"Start date of work.\",\n",
    "            examples=[(\"Дата начала работ 15 мая 2023 года\\nДата завершения проекта 31 июля 2023 года\", \"15.05.2023\"), \n",
    "                      (\"Дата запланированной поставки: 2023-04-30 (первая партия)\", \"30.04.2023\"),\n",
    "                      (\"Дата запланированной поставки: первая партия: BZ: 2024-07-30; AD: 31 мая 2024 года.\", \"30.07.2024\"),\n",
    "                      (\"Проект &lt; &lt; Старый ветер в Синьцзян-Тауне &gt; &gt; : 20 марта 2023 года (в соответствии с письменным уведомлением министерства)\", \"20.03.2023\"),\n",
    "                      (\"Дата завершения проекта: 30 октября 2023 года\", \"30.10.2023\")\n",
    "                      ]\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"end_date\",\n",
    "            description=\"End date of work.\",\n",
    "            examples=[(\"Дата начала работ 15 мая 2023 года\\nДата завершения проекта 31 июля 2023 года\", \"31.06.2023\"), \n",
    "                      ]\n",
    "        ),\n",
    "    ],\n",
    "    examples=[\n",
    "        (\n",
    "            \"\"\"\n",
    "            . ОБЩИЙ ОБЗОР ПРОЕКТОВ И ОГРАНИЧЕНИЙ\n",
    "            Описание проекта: Закупка потребностей в исследованиях и разработках для проекта экспериментальной платформы электрической реле для защиты электрической реле, которая в основном предусматривала проведение научно-исследовательского проекта\n",
    "            Программа включает общие требования к проектированию, материалам, изготовлению оборудования, проверке, испытанию, идентификации, оценке и контролю за документацией. Услуги или предметы\n",
    "            Поставщики (далее \"В\"), помимо требований настоящего Технического регламента, должны отвечать существующим стандартам в стране и отрасли.\n",
    "            Соответствующие положения.\n",
    "            Содержание и сфера охвата тендера: в ходе конкурса было разработано 7 сканеров и комплектов для сканирования аптечек, шкафов и т.д., а также предложено\n",
    "            Услуги по установке и отладке аппаратного обеспечения.\n",
    "            Единица измерения количества серийных номеров\n",
    "            Анализ потребностей в оборудовании 1\n",
    "            2 аппаратных платформы для разработки 1\n",
    "            1 шкаф для проверки 3 локомотивов и 1 шкаф для защиты маршрутов\n",
    "            Оборудование для переключателя переключателя, защищенные карты\n",
    "            14 единиц\n",
    "            шкаф\n",
    "            5 защитных карточек 1 шкаф\n",
    "            1 шкаф для 6 карт защиты давления\n",
    "            1 шкаф для 7-дюймовых магнитных карт\n",
    "            8 универсальных реле 1 шкаф\n",
    "            1 шкаф для 9 универсальных трансформаторов\n",
    "            10 программных интерфейсов для разработки 1\n",
    "            11 тестов для 1\n",
    "            12 Установка и отладка.\n",
    "            1\n",
    "            13 документов, написанных сверху\n",
    "            Дата запланированной поставки: 15 августа 2023 года - 31 декабря 2023 года.\n",
    "            Общий срок службы: 138 календарных дней.\n",
    "            \"\"\",\n",
    "            [\n",
    "                {\"description\": 'Закупка потребностей в исследованиях и разработках для проекта экспериментальной платформы электрической реле для защиты электрической реле, которая в основном предусматривала проведение научно-исследовательского проекта\\nПрограмма включает общие требования к проектированию, материалам, изготовлению оборудования, проверке, испытанию, идентификации, оценке и контролю за документацией. Услуги или предметы\\nПоставщики (далее \"В\"), помимо требований настоящего Технического регламента, должны отвечать существующим стандартам в стране и отрасли.', \n",
    "                 \"requirements\": 'в ходе конкурса было разработано 7 сканеров и комплектов для сканирования аптечек, шкафов и т.д., а также предложено\\nУслуги по установке и отладке аппаратного обеспечения.',\n",
    "                 \"stages\":\"1\",\n",
    "                 \"delivery_time\": \"138\",\n",
    "                 \"start_date\": \"15.08.2023\",\n",
    "                 \"end_date\": \"31.12.2023\",\n",
    "                 },\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    many=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_extraction_chain(llm, schema, encoder_or_encoder_class='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i_text in tqdm(df['part_2'][325:]):\n",
    "    try:\n",
    "        ans = chain.run(text=(i_text))\n",
    "        result.append(ans['data'])\n",
    "    except:\n",
    "        result.append({'project_info': [ {\"description\": '', \n",
    "                                           \"requirements\": '',\n",
    "                                           \"stages\":\"\",\n",
    "                                           \"delivery_time\": \"\",\n",
    "                                           \"start_date\": \"\",\n",
    "                                           \"end_date\": \"\",}]}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for i in tqdm(range(len(result))):\n",
    "    try:\n",
    "        if len(result[i]) > 0:\n",
    "            temp_list.append(result[i]['project_info'][0])\n",
    "        else:\n",
    "            temp_list.append({'description': '',\n",
    "                            'requirements': '',\n",
    "                            'stages': '',\n",
    "                            'delivery_time': '',\n",
    "                            'start_date': '',\n",
    "                            'end_date': ''})\n",
    "    except:\n",
    "        temp_list.append({'description': '',\n",
    "                'requirements': '',\n",
    "                'stages': '',\n",
    "                'delivery_time': '',\n",
    "                'start_date': '',\n",
    "                'end_date': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df.to_csv('./data/part_2_for_merge.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['part_3'] = [x.split('III')[-1] for x in df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема для третьего и последующих разделов applic:\n",
    "* Информация о базовой квалификации кандидата\n",
    "* Информация о квалификационных требованиях\n",
    "* Финансовые требования\n",
    "* Требования к произволственным мощностям\n",
    "* Требования к финансовой репутации\n",
    "* Дата начала тендера\n",
    "* Дата окончания тендера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Object(\n",
    "    id=\"application_data\",\n",
    "    description=\"Tender application data\",\n",
    "    attributes=[\n",
    "        Text(\n",
    "          id='basic_qualification',\n",
    "          description=\"Basic candidate qualifications\",\n",
    "          examples=[(\"\"\"участник процедур, зарегистрированный на территории Китайской Народной Республики в соответствии с Законом Китайской Народной Республики о компаниях, имеет право\n",
    "            Законодательная власть, предприятия, которые могут предложить продукты и услуги для проекта.\"\"\",\n",
    "                    \"\"\"участник процедур должен быть зарегистрирован на территории Китайской Народной Республики в соответствии с Законом Китайской Народной Республики о компаниях\n",
    "            Независимые юридические лица, предприятия, способные предлагать продукты и услуги для проекта.\"\"\")],\n",
    "            ),\n",
    "        Text(\n",
    "          id='qualification_requirements',\n",
    "          description=\"Qualification requirements\",\n",
    "          examples=[(\"\"\"Генеральный контрактный уровень (или уровень А) и выше в области электротехники.\"\"\",\n",
    "                    \"\"\"Для участия в торгах требуется, чтобы участники торгов обладали такими качествами, как общий контракт на строительство гидроэлектроэнергии и выше. (если в соответствии с\n",
    "Министерство по вопросам городского и сельского строительства в связи с выпуском циркуляра о программе реформы системы управления инженерно-строительными предприятиями в целях повышения квалификации или новых сертификатов для гидроэлектроэнергетики\n",
    "Генеральный контракт на строительство уровня А и выше).\"\"\")],\n",
    "            ),\n",
    "        Text(\n",
    "          id='financial_requirements',\n",
    "          description=\"Financial requirements\",\n",
    "          examples=[(\"\"\"Финансовые требования: аудиторские заключения и финансовые ведомости, представленные зарегистрированной аудиторской фирмой за последние три года (2020-2022 годы)\n",
    "балансовая ведомость, ведомость прибылей, ведомость движения денежной наличности и приложение.\"\"\",\n",
    "                    \"\"\"Отчет о финансовой ревизии за последние три года (2019-2021).\"\"\")],\n",
    "            ),\n",
    "        Text(\n",
    "          id='performance_requirements',\n",
    "          description=\"Performance requirements\",\n",
    "          examples=[(\"\"\"должны иметь историю и результативность как минимум на три года производства товаров или аналогичных товаров. (Оборудование для замены коробки должно быть\n",
    "            Комбинированная или сборная с емкостью 3150 КВА и выше, электрическое давление на 35 КВ и выше.\"\"\",\n",
    "                    \"\"\"в течение почти трех лет (с момента подачи тендерной документации, по крайней мере, одного крана и поставки\n",
    "Установка работы.\"\"\")],\n",
    "            ),\n",
    "        Text(\n",
    "          id='credit_requirements',\n",
    "          description=\"Credit requirements\",\n",
    "          examples=[(\"\"\" участники торгов имеют хорошую банковскую и коммерческую репутацию, не находятся в ситуации, когда им предписывается прекратить работу, захват имущества, замораживание\n",
    "            Конкретно, несостоятельно и не находится в пределах и в течение срока, в течение которого компания &lt; &lt; Чайна холдинг компани лимитед &gt; &gt; ограничила свои предложения.\"\"\",\n",
    "                    \"\"\"не включен в систему раскрытия информации о государственных предприятиях (www.gsxt.gov.cn)\n",
    "                      В список и на веб-сайте «Доверие в Китай» (http://www.creditchina.gov.cn/) не было внесено в список «Посланник с неверием».\"\"\")],\n",
    "            ),\n",
    "        Text(\n",
    "          id='tender_start_date',\n",
    "          description=\"Start date for receiving tender documentation\",\n",
    "          examples=[(\"\"\"07 июня 2023 года, 17:00\"\"\",\n",
    "                    \"\"\"28 декабря 2022 года в 17 ч. 00 м.\"\"\")],\n",
    "            ),\n",
    "        Text(\n",
    "          id='tender_end_date',\n",
    "          description=\"End date for receiving tender documentation\",\n",
    "          examples=[(\"\"\"16 июня 2023 года, 17:00\"\"\",\n",
    "                    \"\"\"06 января 2023 года 17 ч. 30 м.\"\"\")],\n",
    "            ),\n",
    "    ],\n",
    "    many=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_extraction_chain(llm, schema,encoder_or_encoder_class='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i_text in tqdm(df['part_3']):\n",
    "    try:\n",
    "        ans = chain.run(text=(i_text))\n",
    "        result.append(ans['data'])\n",
    "    except:\n",
    "        result.append({'basic_qualification': '', 'qualification_requirements': '',\n",
    "   'financial_requirements': '', 'performance_requirements': '', 'credit_requirements': '', \n",
    "   'tender_start_date': '',\n",
    "   'tender_end_date': '',})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for i in tqdm(range(len(result))):\n",
    "    if len(result[i]) > 0:\n",
    "        temp_list.append(result[i]['application_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df.to_csv('./data/part3_for_merge_PART.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Схема для текстов из meta:\n",
    "* Дата заявки\n",
    "* Идентификатор тендера\n",
    "* Краткое описание\n",
    "* Подробное описание \n",
    "* Тип исполнителя\n",
    "* Имя исполнителя \n",
    "* Валюта договора\n",
    "* Наименование заказчика\n",
    "* Тип заказчика\n",
    "* Агент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Object(\n",
    "    id=\"meta_data\",\n",
    "    description=\"Tender meta data\",\n",
    "    attributes=[\n",
    "        Text(\n",
    "          id='release_date',\n",
    "          description=\"Tender release date\",\n",
    "          examples=[(\"2023-06-27\", \"2023-04-11\")],\n",
    "        ),\n",
    "        Text(\n",
    "          id='CGN',\n",
    "          description=\"CGN of the tender\",\n",
    "          examples=[(\"CGN-202304240017\", \"CGN-20180529004\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"brief_description\",\n",
    "            description=\"Brief description of the tender task\",\n",
    "            examples=[(\"\"\"Дороги, платформы для подвески, фундаментальные работы в ветряных машинах\n",
    "                            Результаты в трех пунктах\"\"\", \n",
    "                       \"Установлены результаты первого из трех башен в морской ветряной трубе на острове Янцзы\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"detailed_description\",\n",
    "            description=\"Detailed description of the tender task\",\n",
    "            examples=[(\"\"\"Тюрьма Си, Хинпхендо, чайный фонтан и электрическая дорога в рамках проекта Ветер , платформа для подвески и три этапа базовой инженерной работы ветроустановок (прибор)\"\"\", \n",
    "                       \"Первый три башни проекта морской ветровой энергетики на Кананге, Китай, и Яньцзян\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"executor_type\",\n",
    "            description=\"Type of tenderer company\",\n",
    "            examples=[(\"Производитель\", \"Победитель\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"executor_name\",\n",
    "            description=\"Name of the company executing the tender\",\n",
    "            examples=[(\"China 19 Metals Group\", \"Hongong Kong\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"price_currency\",\n",
    "            description=\"Currency of the tender price\",\n",
    "            examples=[(\"¥\",\"RMB\")]\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"customer_name\",\n",
    "            description=\"Name of company customer of the tender\",\n",
    "            examples=[(\"Ветроэнергетика, Чайна\", \"Чайна инжиниринг лимитед\")],\n",
    "        ),\n",
    "        Text(\n",
    "            id=\"customer_type\",\n",
    "            description=\"Type of tender customer company\",\n",
    "            examples=[(\"Участники торгов\", \"Заявители\")],\n",
    "        ),  \n",
    "        Text(\n",
    "            id=\"bidding_agent\",\n",
    "            description=\"Bidding Agent\",\n",
    "            examples=[(\"Чайна инжиниринг лимитед\", \"Чайна инжиниринг лимитед\")],\n",
    "        ), \n",
    "    ],\n",
    "    examples=[\n",
    "        (\n",
    "            \"\"\"\n",
    "            Дата выхода: 2023-06-27\n",
    "            Дороги, платформы для подвески, фундаментальные работы в ветряных машинах\n",
    "            Результаты в трех пунктах\n",
    "            (Контингент: CGN-202304240017)\n",
    "            Тюрьма Си, Хинпхендо, чайный фонтан и электрическая дорога в рамках проекта &lt; &lt; Ветер &gt; &gt; , платформа для подвески и три этапа базовой инженерной работы ветроустановок (прибор)\n",
    "            Номер проекта: CGN-202304240017, идентифицированные лица:\n",
    "            1. Информация о кандидатах:\n",
    "            Производитель: China 19 Metals Group\n",
    "            Цены: ¥33 750 545.00\n",
    "            II. ДРУГИЕ ОБЯЗАТЕЛЬСТВА:\n",
    "            Нет\n",
    "            III. УПРАВЛЕНИЕ ВЕРХОВНОГО КОМИССАРА\n",
    "            Надзор за осуществлением данного тендерного проекта /.\n",
    "            IV. Связи\n",
    "            Участники торгов: &lt; &lt; Ветроэнергетика &gt; &gt; , &lt; &lt; Чайна &gt; &gt;\n",
    "            Контактное лицо: джентльмен\n",
    "            Тел.: 010-63711966\n",
    "            Электронная почта: guobijun@cgnpc.com\n",
    "            Агент по торгам: &quot; Чайна инжиниринг лимитед &quot;\n",
    "            Контактное лицо: Ли Хун\n",
    "            Тел.: 0755-84436511\n",
    "            Электронная почта:\n",
    "            lhyts@cgnpc.com.cn\n",
    "            Участник торгов или его главный руководитель (руководитель проекта): (подпись)\n",
    "            Участник торгов или его агент: (глава)\n",
    "            \"\"\",\n",
    "            [\n",
    "                {'release_date':\"2023-06-27\", \n",
    "                 'CGN':\"CGN-202304240017\", \n",
    "                 \"brief_description\":\"\"\"Дороги, платформы для подвески, фундаментальные работы в ветряных машинах\n",
    "Результаты в трех пунктах\"\"\", \n",
    "                 \"detailed_description\":\"\"\"Тюрьма Си, Хинпхендо, чайный фонтан и электрическая дорога в рамках проекта Ветер, платформа для подвески и три этапа базовой инженерной работы ветроустановок (прибор)\"\"\", \n",
    "                 \"executor_type\":\"Производитель\", \n",
    "                 \"executor_name\":\"China 19 Metals Group\", \n",
    "                 \"price_currency\":\"¥\",\n",
    "                 \"customer_name\":\"Ветроэнергетика, Чайна\", \n",
    "                 \"customer_type\":\"Участники торгов\", \n",
    "                 \"bidding_agent\":\"Чайна инжиниринг лимитед\"}\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"\"\"\n",
    "            Дата выхода: 2023-04-11\n",
    "            Установлены результаты первого из трех башен в морской ветряной трубе на острове Янцзы\n",
    "            (Контингент: CGN-20180529004)\n",
    "            Первый три башни проекта морской ветровой энергетики на Кананге, Китай, и Яньцзян (заявление No CGN-20180529004)\n",
    "            Установлено следующее:\n",
    "            1. Информация о кандидатах:\n",
    "            Hongong Kong\n",
    "            Победитель:\n",
    "            Цены: RMB14 068 380.00\n",
    "            II. ДРУГИЕ ОБЯЗАТЕЛЬСТВА:\n",
    "            III. УПРАВЛЕНИЕ ВЕРХОВНОГО КОМИССАРА\n",
    "            .\n",
    "            Наблюдение за осуществлением данного тендерного проекта /\n",
    "            IV. Связи\n",
    "            Заявители: &quot; Чайна инжиниринг лимитед &quot;\n",
    "            Местонахождение:/\n",
    "            Организация: двойная\n",
    "            Телефон: 0755-88616541\n",
    "            Электронная почта: luozhenggang@cgnpc.com.cn\n",
    "            Агент по торгам: &quot; Чайна инжиниринг лимитед &quot;\n",
    "            Местонахождение:/\n",
    "            Организация: Худо\n",
    "            Телефон: 0755-88610728\n",
    "            Электронная почта:huduo@cgnpc.com.cn\n",
    "            Участник торгов или его главный руководитель (руководитель проекта): (подпись)\n",
    "            Участник торгов или его агент: (глава)\n",
    "            \"\"\",\n",
    "            [\n",
    "                {'release_date':\"2023-04-11\", \n",
    "                 'CGN':\"CGN-20180529004\", \n",
    "                 \"brief_description\":\"\"\"Установлены результаты первого из трех башен в морской ветряной трубе на острове Янцзы\"\"\", \n",
    "                 \"detailed_description\":\"\"\"Первый три башни проекта морской ветровой энергетики на Кананге, Китай, и Яньцзян\"\"\", \n",
    "                 \"executor_type\":\"Победитель\", \n",
    "                 \"executor_name\":\"Hongong Kong\", \n",
    "                 \"price_currency\":\"RMB\",\n",
    "                 \"customer_name\":\"Чайна инжиниринг лимитед\", \n",
    "                 \"customer_type\":\"Заявители\", \n",
    "                 \"bidding_agent\":\"Чайна инжиниринг лимитед\"}\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    many=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_extraction_chain(llm, schema,encoder_or_encoder_class='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i_text in tqdm(df['meta']):\n",
    "    ans = chain.run(text=(i_text))\n",
    "    result.append(ans['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for i in tqdm(range(len(result))):\n",
    "    if len(result[i]) > 0:\n",
    "        temp_list.append(result[i]['meta_data'][0])\n",
    "    else:\n",
    "     temp_list.append({'release_date':\"\", \n",
    "                 'CGN':\"\", \n",
    "                 \"brief_description\":\"\", \n",
    "                 \"detailed_description\":\"\", \n",
    "                 \"executor_type\":\"\", \n",
    "                 \"executor_name\":\"\", \n",
    "                 \"price_currency\":\"\",\n",
    "                 \"customer_name\":\"\", \n",
    "                 \"customer_type\":\"\", \n",
    "                 \"bidding_agent\":\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
